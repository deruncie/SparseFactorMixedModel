\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{bm}
\newcommand*{\B}[1]{\ifmmode\bm{#1}\else\textbf{#1}\fi}
\DeclareMathOperator{\E}{\mathbb{E}}

\title{Brief Article}
\author{The Author}
%\date{}                                           % Activate to display a given date or no date

\begin{document}

\subsection{Notes}
ideas from reading Piironen and Vehtari (2017):
\begin{itemize}
	\item $\beta$ should be proportional to $\sigma$. This is important for setting the prior with m\textsubscript{eff}.
	\item The calculation of  m\textsubscript{eff} is somewhat different for regularized horseshoe
	\item A Gibbs sampler is fine for the regularized horseshoe thinking of it as a two-part prior for $\beta$
	\item Should be able to use the Makalic / Rajaratnam method for simultaneously sampling $\beta$ and $\sigma^2$. This is not hugely more computationally difficult that sampling them separately. Does it help when $p << n$? Probably most helpful for eQTL.
	\item prior for $c$ is still challenging. I think that the prior for the variance explained by $\bm{X}\bm{\beta}$ should be  $\mbox{m\textsubscript{eff}} \times c^2$, because the approximately m\textsubscript{eff} $\beta_j$ are approximatly distributed as $\mbox{N}(0,c^2)$. So, If we want the total variance to be 1 and fraction $\pi$ to be from $\bm{X}\bm{\beta}$, we should set $c = \pi / \mbox{m\textsubscript{eff}}$, and $\sigma^2 = (1-\pi)$.
	\item This does mean that when $\tau^2 \rightarrow 0$ in the prior for $\beta_j$, the total variance of the factor will go to $(1-\pi)$. This suggests that maybe I don't want both $\tau$ and $pi$ to be parameters? But I do: I want both a prior on the fraction of variation explained by X, and a separate prior on the number of non-zero $\beta_j$. Maybe a prior with a large weight on $\pi = 0$ would address this? This would effectively ``turn off" $\bm{X\beta}$ for some factors. I think that as long as the prior on is decreasing, then if $\tau \rightarrow 0$, then $pi \rightarrow 0$.
	\item It appears that the prior on the variance of $\bm{X\beta}$ is approximately $\mbox{m\textsubscript{eff}} \times c^2$ when m\textsubscript{eff} is large, but $\mbox{m\textsubscript{eff}} \times c$ when m\textsubscript{eff} is small.
	\item What I'd like is for the prior to be on the variance of $\bm{X\beta}$ to be independent of $\tau$, so that we can explain the same percentage of variance with a few large effects or a bunch of small effects. This means that $c$ needs to be a function of $\tau$.
	\item This would be hard to do directly, because then the posterior of $\tau$ would depend on $c$, and this is unlikely to be conjugate.
	\item But, if $c$ is specific to each factor and $\tau$ is general across factors, then this isn't a huge problem. At least, it should be constant across factors.
	\
\end{itemize}



We propose an ``Sparse Infinite Factor Model" based on the horseshoe prior. 
The horsehoe prior is a popular choice for inducing approximate sparsity in the coefficients of a regression model. 
Bhattarcharya and Dunson (2011) proposed an ``Infinite factor model" by ordering the latent factors from most-to-least
influential, and then modeling the decrease in factor importance from one factor to the next as a stochastic process. Using this strategy,
they showed that as long as they included a sufficiently large number of factors are included, their model could automatically prioritize 
only the most important factors, allowing the insignificant ones to be truncated.

In their implementation of the ``Infinite Factor Model," Bhattarcharya and Dunson (2011) used the Normal-InverseGamma prior on the 
coefficients of the loadings matrix $\B\Lambda$, and used a sequence generated by the product of gamma-distributed to induce additional shrinkage on the coefficients of each column of the matrix. Here, we propose two alterations to this model: i) We use the horseshoe prior on the individual coefficients of $\B\Lambda$ to induce more complete shrinkage. ii) We use a half-Cauchy distribution with stochastically decreasing scale parameter to model the column-shrinkage, and parameterize this distribution based on prior belief in the proportion of non-zero entries in each column. In particular, we place a prior on the proportion of non-zero values in the first column of $\B\Lambda$, and then a second prior on the change in odds of being non-zero for each coefficient in each subsequent column. Together, these modifications allow us to parameterize our model in terms of the effective number of non-zero coefficients in each factor.

Below, we show our new factor model and derive a Gibbs sampler for the parameters.
Following Makalic and Schmidt (2015) and Piironen and Vehtari (2017), the horseshoe model for a one-dimensional trait$\bf y$  with $n$ observations and $\B X$ known is:

\begin{align}
\begin{split}
\label{horseshoe_regression}
\bf y \mid \B{X},\B{\beta},\sigma^2 &\sim \mbox{N}(\B{X\beta},\sigma^2\B{I}_n), \\
\beta_j \mid \lambda^2_j, \tau^2, \sigma^2 &\sim \mbox{N}(0,\lambda^2_j \tau^2 \sigma^2), \\
\sigma^2 &\sim \sigma^{-2}d\sigma^2, \\
\lambda_j &\sim \mbox{C}^+(0,1), \\
\tau &\sim \mbox{C}^+(0,\tau_o)
\end{split}
\end{align}

In our factor model, each column of the data matrix $\B Y$ is independent conditioned on the factor matrix $\B F$, with distribution:
\begin{align*}
\bf{y}_j \mid \B{F},\B{\lambda}_j,\sigma^2 &\sim \mbox{N}(\B{F}\B{\lambda}_j, \sigma^2_j\B{\Sigma}_j).
\end{align*}

We propose the following model  for the coefficients of $\B\Lambda = [\B{\lambda}_j], j \in 1,\dots,p$:
\begin{align}
\begin{split}
\label{horseshoe_factor}
\lambda_{kj} \mid \phi^2_{kj}, \tau^2_k, \sigma^2_j &\sim \mbox{N}(0,\phi^2_{kj}\omega^2\tau^{-1}_k \sigma^2_j), \\
\sigma^2_j &\sim \mbox{IG}(a,b), \\
\phi_{kj} &\sim \mbox{C}^+(0,1), \\
\omega = &\sim \mbox{C}^+(0,\omega^2_o), \\
\tau_k &= \prod\limits_{h=1}^k \delta_h, \\
\delta_h &\sim \mbox{Ga}(a_\delta,b_\delta), h \geq 1, \; \delta_1 = 1.
\end{split}
\end{align}

This model implements the local-global shrinkage property of the horseshoe prior on each column of $\B \Lambda$, but with a stochastically decreasing global shrinkage parameter given by the synthetic parameter $\tilde{\omega}^2 = \omega^2\tau^{-1}_k$. We can interpret this parameter as follows:

Piironen and Vehtari (2017) show that the effective number of parameters in a horseshoe model~\ref{horseshoe_regression} can be approximated based on the shrinkage factors $\kappa_j = (1 + n\sigma^{-2}\tau^2\lambda_j^2)$ as:
$$m_{\mbox{eff}} = \sum \limits_{j=1}^D (1-\kappa_j).$$

\noindent They calculate the mean of $m_{\mbox{eff}}$ as:

$$\E [m_{\mbox{eff}} \mid \tau,\sigma] = \frac{\sigma^{1}\tau\sqrt{n}}{1 + \sigma^{-1}\tau\sqrt{n}}D.$$
\noindent and suggest a prior for $\tau$ with most of its mass near the value:
$$\tau_o = \frac{p_o}{1-p_o} \frac{\sigma}{\sqrt{n}},$$
\noindent where $p_o$ is the prior guess at the number of non-zero coefficients. The first term on the RHS of this equation can be interpreted as the odds of inclusion for any coefficient. A reasonable choice for this prior is thus $\tau \sim \mbox{C}^+(0,\tau^2_o)$. Using a similar argument, we can set:
$$\omega_o = \frac{p_o}{1-p_o} \frac{\sigma}{\sqrt{n}},$$
\noindent where $\frac{p_o}{1-p_o}$ is the odds of includion for each coefficient \textit{in the first column of} $\B\Lambda$, and $\sigma$ is the prior guess for the typical residual standard deviation across traits. This follows because $\tilde{\omega}^2_1 = \omega^2\tau^{-1}_1 = \omega^2$.

Now, for the $k$th column of $\B\Lambda$, we have 
$$\tilde{\omega}^2_k = \omega^2\tau^{-1}_k = \omega^2 \prod\limits_{h=1}^k \delta_h = \omega^2_{k-1} \delta^{-1}_k.$$
\noindent If $\tilde{\omega}_1$ is centered around $\omega_o$, then $\tilde{\omega}_2$ is centered around:
$$\omega_o  \frac{1}{\sqrt{\delta_2}} = \left( \frac{p_o}{1-p_o} \frac{1}{\sqrt{\delta_2}}\right)\frac{\sigma}{\sqrt{n}}$$. 

Therefore, the expected odds of inclusion for a coefficient in the 2nd column of $\B\Lambda$ is reduced by a factor of $\sqrt{\delta_2}$ relative to the first. The same result will hold for the odds of inclusion for each coefficient in each subsequent column, conditional on the odds of inclusion for each coefficient of the previous column. This provides a tool for calibrating the prior on $\delta_k$ based on the expected rate of decline of the number of non-zero coefficients of each subsequent factor in the model. A prior that places most mass on $\delta_k > 1$ will have stochastically increasing shrinkange on higher-order columns.

To implement this ``Sparse Infinite Horseshoe Factor Model," we re-parameterize it for Gibbs sampling following Makalic and Schmidt (2015) as:


\begin{align*}
y \mid \B{F},\B{\lambda}_j,\sigma^2 &\sim \mbox{N}(\B{F}\B{\lambda}_j, \sigma^2_j\B{\Sigma}_j), \\
\lambda_{kj} \mid \phi^2_{kj}, \tau^2_k, \sigma^2_j &\sim \mbox{N}(0,\phi^2_{kj} \omega^2 \tau_k^{-1} \sigma^2_j), \\
\sigma^2_j &\sim \mbox{IG}(a,b), \\
\phi^2_{kj} \mid \nu_{kj} &\sim \mbox{IG}(1/2,1/\nu_{kj}), \\
\nu_{kj} &\sim \mbox{IG}(1/2,1), \\
\omega^2 \mid \xi &\sim \mbox{IG}(1/2,1/\xi), \\
\xi &\sim \mbox{IG}(1/2,1), \\
\tau_k &= \prod\limits_{h=1}^k \delta_h, \\
\delta_h &\sim \mbox{Ga}(a_\delta,b_\delta), h \geq 1, \; \delta_1 = \omega^{-2}_o.
\end{align*}

All priors in this model are conjugate, allowing Gibbs updates. 

\end{document}  